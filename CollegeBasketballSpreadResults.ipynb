{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3e8c5b-5c5c-4a6e-9eb2-a665ae57af76",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 15) (2395782031.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    QUERY=\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 15)\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Bash script to calculate homefield advantage using SSH tunnel and MySQL\n",
    "\n",
    "echo \"Starting SSH tunnel...\"\n",
    "ssh -L 10022:127.0.0.1:3306 forge@67.205.170.38 -N -f\n",
    "SSH_PID=$!\n",
    "\n",
    "# Wait for tunnel to establish\n",
    "sleep 2\n",
    "\n",
    "echo \"Running analysis query...\"\n",
    "\n",
    "# SQL query to calculate homefield advantage\n",
    "QUERY=\"\n",
    "WITH latest_spreads AS (\n",
    "    SELECT \n",
    "        sr.game_id,\n",
    "        sr.fpi_spread\n",
    "    FROM \n",
    "        spread_results sr\n",
    "    INNER JOIN (\n",
    "        SELECT \n",
    "            game_id, \n",
    "            MAX(recorded_at) as max_recorded_at\n",
    "        FROM \n",
    "            spread_results\n",
    "        GROUP BY \n",
    "            game_id\n",
    "    ) latest ON sr.game_id = latest.game_id AND sr.recorded_at = latest.max_recorded_at\n",
    "),\n",
    "game_analysis AS (\n",
    "    SELECT \n",
    "        ls.game_id,\n",
    "        ls.fpi_spread,\n",
    "        s.home_score,\n",
    "        s.away_score,\n",
    "        (s.away_score - s.home_score) AS actual_spread,\n",
    "        (ls.fpi_spread - (s.away_score - s.home_score)) AS prediction_error\n",
    "    FROM \n",
    "        latest_spreads ls\n",
    "    INNER JOIN \n",
    "        scores s ON ls.game_id = s.game_id\n",
    ")\n",
    "SELECT \n",
    "    COUNT(*) AS sample_size,\n",
    "    AVG(prediction_error) AS mean_error,\n",
    "    \n",
    "    -- For MySQL 8.0+ (uncomment if your MySQL version supports it)\n",
    "    -- PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY prediction_error) AS median_homefield_advantage,\n",
    "    \n",
    "    -- For older MySQL versions or simpler approach\n",
    "    -- This is an approximation of median\n",
    "    (\n",
    "        SELECT AVG(t1.prediction_error) \n",
    "        FROM game_analysis t1, game_analysis t2 \n",
    "        GROUP BY t1.prediction_error \n",
    "        HAVING SUM(CASE WHEN t2.prediction_error <= t1.prediction_error THEN 1 ELSE 0 END) >= COUNT(*)/2 \n",
    "        AND SUM(CASE WHEN t2.prediction_error >= t1.prediction_error THEN 1 ELSE 0 END) >= COUNT(*)/2 \n",
    "        LIMIT 1\n",
    "    ) AS median_homefield_advantage,\n",
    "    \n",
    "    STDDEV(prediction_error) AS std_error\n",
    "FROM \n",
    "    game_analysis;\n",
    "\"\n",
    "\n",
    "# Run query through mysql client\n",
    "mysql -h 127.0.0.1 -P 10022 -u forge forge -e \"$QUERY\" > results.txt\n",
    "\n",
    "# Display the results\n",
    "echo \"===== FPI Homefield Advantage Analysis =====\"\n",
    "cat results.txt\n",
    "\n",
    "# Save the data for further analysis\n",
    "echo \"Saving sample data for analysis...\"\n",
    "mysql -h 127.0.0.1 -P 10022 -u forge forge -e \"\n",
    "    SELECT \n",
    "        ls.game_id,\n",
    "        ls.fpi_spread,\n",
    "        s.home_score,\n",
    "        s.away_score,\n",
    "        (s.away_score - s.home_score) AS actual_spread,\n",
    "        (ls.fpi_spread - (s.away_score - s.home_score)) AS prediction_error\n",
    "    FROM (\n",
    "        SELECT \n",
    "            sr.game_id,\n",
    "            sr.fpi_spread\n",
    "        FROM \n",
    "            spread_results sr\n",
    "        INNER JOIN (\n",
    "            SELECT \n",
    "                game_id, \n",
    "                MAX(recorded_at) as max_recorded_at\n",
    "            FROM \n",
    "                spread_results\n",
    "            GROUP BY \n",
    "                game_id\n",
    "        ) latest ON sr.game_id = latest.game_id AND sr.recorded_at = latest.max_recorded_at\n",
    "    ) ls\n",
    "    INNER JOIN \n",
    "        scores s ON ls.game_id = s.game_id\n",
    "    LIMIT 100;\n",
    "\" > fpi_sample_data.txt\n",
    "\n",
    "echo \"Sample data saved to fpi_sample_data.txt\"\n",
    "\n",
    "# Clean up by closing the SSH tunnel\n",
    "echo \"Closing SSH tunnel...\"\n",
    "kill $SSH_PID\n",
    "\n",
    "echo \"Analysis complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c3f395-b8a3-4fae-9990-9a49c2e5956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paramiko\n",
      "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sshtunnel\n",
      "  Downloading sshtunnel-0.4.0-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/anaconda3/lib/python3.11/site-packages (from paramiko) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/anaconda3/lib/python3.11/site-packages (from paramiko) (42.0.2)\n",
      "Collecting pynacl>=1.5 (from paramiko)\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: cffi>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from bcrypt>=3.2->paramiko) (1.16.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from bcrypt>=3.2->paramiko) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.1->bcrypt>=3.2->paramiko) (2.21)\n",
      "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sshtunnel-0.4.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl (349 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.9/349.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pynacl, paramiko, sshtunnel\n",
      "Successfully installed paramiko-3.5.1 pynacl-1.5.0 sshtunnel-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install paramiko sshtunnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49326b67-4441-4ea6-816c-6fcb8735205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing SSH tunnel...\n",
      "SSH tunnel established.\n",
      "Database connection established.\n",
      "\n",
      "=== Tables in the database ===\n",
      "          Tables_in_forge\n",
      "0                   cache\n",
      "1             cache_locks\n",
      "2                 casinos\n",
      "3    early_access_signups\n",
      "4             failed_jobs\n",
      "5             fpi_ratings\n",
      "6                   games\n",
      "7             job_batches\n",
      "8                    jobs\n",
      "9              migrations\n",
      "10     money_line_results\n",
      "11            money_lines\n",
      "12            nba_margins\n",
      "13          ncaab_margins\n",
      "14          ncaaf_margins\n",
      "15            nfl_margins\n",
      "16     over_under_results\n",
      "17            over_unders\n",
      "18  password_reset_tokens\n",
      "19                 scores\n",
      "20               sessions\n",
      "21                 sports\n",
      "22         spread_results\n",
      "23                spreads\n",
      "24          subscriptions\n",
      "25                  teams\n",
      "26                  users\n",
      "27               visitors\n",
      "\n",
      "=== Columns in spread_results table ===\n",
      "                      Field                                        Type Null  \\\n",
      "0                        id                             bigint unsigned   NO   \n",
      "1                 spread_id                             bigint unsigned   NO   \n",
      "2                  score_id                             bigint unsigned   NO   \n",
      "3                    result  enum('home_covered','away_covered','push')   NO   \n",
      "4                created_at                                   timestamp  YES   \n",
      "5                updated_at                                   timestamp  YES   \n",
      "6                fpi_spread                                decimal(4,1)  YES   \n",
      "7       fpi_adjusted_spread                                decimal(8,1)  YES   \n",
      "8     fpi_spread_difference                                      double  YES   \n",
      "9   fpi_correctly_predicted                                  tinyint(1)  YES   \n",
      "10   fpi_better_than_spread                                  tinyint(1)  YES   \n",
      "\n",
      "    Key Default           Extra  \n",
      "0   PRI    None  auto_increment  \n",
      "1   MUL    None                  \n",
      "2   MUL    None                  \n",
      "3          None                  \n",
      "4          None                  \n",
      "5          None                  \n",
      "6          None                  \n",
      "7          None                  \n",
      "8          None                  \n",
      "9          None                  \n",
      "10         None                  \n",
      "\n",
      "=== Sample data from spread_results ===\n",
      "      id  spread_id  score_id        result          created_at  \\\n",
      "0  34601    4819876       486  home_covered 2025-03-03 04:58:01   \n",
      "\n",
      "           updated_at  fpi_spread  fpi_adjusted_spread  fpi_spread_difference  \\\n",
      "0 2025-03-03 04:58:01         2.2                 -1.3                    2.8   \n",
      "\n",
      "   fpi_correctly_predicted  fpi_better_than_spread  \n",
      "0                        1                       1  \n",
      "\n",
      "=== Columns in scores table ===\n",
      "        Field                            Type Null  Key Default  \\\n",
      "0          id                 bigint unsigned   NO  PRI    None   \n",
      "1     game_id                 bigint unsigned   NO  MUL    None   \n",
      "2  home_score                             int   NO         None   \n",
      "3  away_score                             int   NO         None   \n",
      "4      period  enum('1','2','3','4','OT','F')   NO         None   \n",
      "5  created_at                       timestamp  YES         None   \n",
      "6  updated_at                       timestamp  YES         None   \n",
      "7    home_fpi                    decimal(8,2)  YES         None   \n",
      "8    away_fpi                    decimal(8,2)  YES         None   \n",
      "9        date                        datetime  YES         None   \n",
      "\n",
      "            Extra  \n",
      "0  auto_increment  \n",
      "1                  \n",
      "2                  \n",
      "3                  \n",
      "4                  \n",
      "5                  \n",
      "6                  \n",
      "7                  \n",
      "8                  \n",
      "9                  \n",
      "\n",
      "=== Sample data from scores ===\n",
      "   id  game_id  home_score  away_score period          created_at  \\\n",
      "0   1     1692          68          80      F 2025-02-11 00:01:00   \n",
      "\n",
      "           updated_at  home_fpi  away_fpi       date  \n",
      "0 2025-02-21 03:49:53      12.0      11.4 2025-02-09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "def check_tables():\n",
    "    # SSH connection parameters\n",
    "    ssh_host = '67.205.170.38'\n",
    "    ssh_port = 22\n",
    "    ssh_username = 'forge'\n",
    "    ssh_password = None  # Using None for key-based authentication\n",
    "    \n",
    "    # Database connection parameters\n",
    "    db_host = '127.0.0.1'  # localhost through the SSH tunnel\n",
    "    db_port = 3306  # Default MySQL port\n",
    "    db_username = 'forge'\n",
    "    db_password = 'kzfSSg5QxhEiK1Y22pwY'\n",
    "    db_name = 'forge'\n",
    "    \n",
    "    try:\n",
    "        print(\"Establishing SSH tunnel...\")\n",
    "        with SSHTunnelForwarder(\n",
    "            (ssh_host, ssh_port),\n",
    "            ssh_username=ssh_username,\n",
    "            ssh_password=ssh_password,\n",
    "            remote_bind_address=(db_host, db_port),\n",
    "            local_bind_address=('127.0.0.1', 10022)\n",
    "        ) as tunnel:\n",
    "            print(\"SSH tunnel established.\")\n",
    "            \n",
    "            # Connect to the database through the SSH tunnel\n",
    "            connection_string = f\"mysql+pymysql://{db_username}:{db_password}@127.0.0.1:{tunnel.local_bind_port}/{db_name}\"\n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            print(\"Database connection established.\")\n",
    "            \n",
    "            # First, let's see what tables are in the database\n",
    "            print(\"\\n=== Tables in the database ===\")\n",
    "            tables_query = \"SHOW TABLES;\"\n",
    "            tables = pd.read_sql(tables_query, engine)\n",
    "            print(tables)\n",
    "            \n",
    "            # Now, let's check the columns in the spread_results table\n",
    "            try:\n",
    "                print(\"\\n=== Columns in spread_results table ===\")\n",
    "                columns_query = \"SHOW COLUMNS FROM spread_results;\"\n",
    "                columns = pd.read_sql(columns_query, engine)\n",
    "                print(columns)\n",
    "                \n",
    "                # Show a sample row\n",
    "                print(\"\\n=== Sample data from spread_results ===\")\n",
    "                sample_query = \"SELECT * FROM spread_results LIMIT 1;\"\n",
    "                sample = pd.read_sql(sample_query, engine)\n",
    "                print(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing spread_results: {e}\")\n",
    "                \n",
    "            # Next, let's check the columns in the scores table\n",
    "            try:\n",
    "                print(\"\\n=== Columns in scores table ===\")\n",
    "                columns_query = \"SHOW COLUMNS FROM scores;\"\n",
    "                columns = pd.read_sql(columns_query, engine)\n",
    "                print(columns)\n",
    "                \n",
    "                # Show a sample row\n",
    "                print(\"\\n=== Sample data from scores ===\")\n",
    "                sample_query = \"SELECT * FROM scores LIMIT 1;\"\n",
    "                sample = pd.read_sql(sample_query, engine)\n",
    "                print(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing scores: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d624981-319d-453e-a907-fbde417d453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ideal homefield advantage...\n",
      "Establishing SSH tunnel...\n",
      "SSH tunnel established.\n",
      "Database connection established.\n",
      "Trying join with score_id...\n",
      "Retrieved 36724 records using score_id join\n",
      "\n",
      "Calculating homefield advantage...\n",
      "Data saved to homefield_advantage_analysis.csv\n",
      "\n",
      "===== ESPN FPI Homefield Advantage Analysis =====\n",
      "Ideal Homefield Advantage: -3.60 points\n",
      "Mean Error: -3.73 points\n",
      "Standard Deviation: 18.13 points\n",
      "Sample Size: 36724 games\n",
      "Error Range: -75.30 to 63.80\n",
      "Interquartile Range: -15.00 to 7.80\n",
      "\n",
      "Interpretation:\n",
      "ESPN FPI is UNDERESTIMATING home field advantage by 3.60 points\n",
      "To correct this, SUBTRACT 3.60 from the FPI spread\n",
      "\n",
      "Example Adjustments:\n",
      "If FPI spread is -7 (home team favored by 7), adjusted spread would be -10.60\n",
      "If FPI spread is +3.5 (away team favored by 3.5), adjusted spread would be -0.10\n",
      "\n",
      "Analysis files saved:\n",
      "- homefield_advantage_analysis.csv (Full data for further analysis)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "def calculate_homefield_advantage():\n",
    "    # SSH and database connection parameters\n",
    "    ssh_host = '67.205.170.38'\n",
    "    ssh_port = 22\n",
    "    ssh_username = 'forge'\n",
    "    db_username = 'forge'\n",
    "    db_password = 'kzfSSg5QxhEiK1Y22pwY'\n",
    "    db_name = 'forge'\n",
    "    \n",
    "    try:\n",
    "        print(\"Establishing SSH tunnel...\")\n",
    "        with SSHTunnelForwarder(\n",
    "            (ssh_host, ssh_port),\n",
    "            ssh_username=ssh_username,\n",
    "            remote_bind_address=('127.0.0.1', 3306),\n",
    "            local_bind_address=('127.0.0.1', 10022)\n",
    "        ) as tunnel:\n",
    "            print(\"SSH tunnel established.\")\n",
    "            \n",
    "            # Connect to the database through the SSH tunnel\n",
    "            connection_string = f\"mysql+pymysql://{db_username}:{db_password}@127.0.0.1:{tunnel.local_bind_port}/{db_name}\"\n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            print(\"Database connection established.\")\n",
    "            \n",
    "            # Try score_id as the join key between spread_results and scores\n",
    "            print(\"Trying join with score_id...\")\n",
    "            query1 = \"\"\"\n",
    "            SELECT \n",
    "                s.id,\n",
    "                s.home_score,\n",
    "                s.away_score,\n",
    "                sr.fpi_spread\n",
    "            FROM \n",
    "                scores s\n",
    "            JOIN \n",
    "                spread_results sr ON s.id = sr.score_id\n",
    "            \"\"\"\n",
    "            \n",
    "            data_df1 = pd.read_sql(query1, engine)\n",
    "            print(f\"Retrieved {len(data_df1)} records using score_id join\")\n",
    "            \n",
    "            # If the first join doesn't work, try game_id as the join key\n",
    "            if len(data_df1) == 0:\n",
    "                print(\"Trying join with game_id...\")\n",
    "                query2 = \"\"\"\n",
    "                SELECT \n",
    "                    s.id,\n",
    "                    s.home_score,\n",
    "                    s.away_score,\n",
    "                    sr.fpi_spread\n",
    "                FROM \n",
    "                    scores s\n",
    "                JOIN \n",
    "                    spread_results sr ON s.game_id = sr.game_id\n",
    "                \"\"\"\n",
    "                \n",
    "                data_df1 = pd.read_sql(query2, engine)\n",
    "                print(f\"Retrieved {len(data_df1)} records using game_id join\")\n",
    "            \n",
    "            # If both joins don't work, try a third option\n",
    "            if len(data_df1) == 0:\n",
    "                print(\"Trying join with id (scores) to spread_id (spread_results)...\")\n",
    "                query3 = \"\"\"\n",
    "                SELECT \n",
    "                    s.id,\n",
    "                    s.home_score,\n",
    "                    s.away_score,\n",
    "                    sr.fpi_spread\n",
    "                FROM \n",
    "                    scores s\n",
    "                JOIN \n",
    "                    spread_results sr ON s.id = sr.spread_id\n",
    "                \"\"\"\n",
    "                \n",
    "                data_df1 = pd.read_sql(query3, engine)\n",
    "                print(f\"Retrieved {len(data_df1)} records using id to spread_id join\")\n",
    "            \n",
    "            # If still no data, let's check for FPI spread values\n",
    "            if len(data_df1) == 0:\n",
    "                print(\"Checking for FPI spread data...\")\n",
    "                fpi_query = \"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as count_with_fpi\n",
    "                FROM \n",
    "                    spread_results\n",
    "                WHERE \n",
    "                    fpi_spread IS NOT NULL\n",
    "                \"\"\"\n",
    "                \n",
    "                fpi_count = pd.read_sql(fpi_query, engine)\n",
    "                print(f\"Found {fpi_count['count_with_fpi'].iloc[0]} records with FPI spread values\")\n",
    "                \n",
    "                # If we have FPI data, the issue might be with the join condition\n",
    "                if fpi_count['count_with_fpi'].iloc[0] > 0:\n",
    "                    # Try a more flexible join approach\n",
    "                    print(\"Checking all possible joins between tables...\")\n",
    "                    # Get columns from both tables\n",
    "                    spread_cols = pd.read_sql(\"SHOW COLUMNS FROM spread_results\", engine)['Field'].tolist()\n",
    "                    scores_cols = pd.read_sql(\"SHOW COLUMNS FROM scores\", engine)['Field'].tolist()\n",
    "                    \n",
    "                    # Look for ID-like columns\n",
    "                    spread_id_cols = [col for col in spread_cols if 'id' in col.lower()]\n",
    "                    scores_id_cols = [col for col in scores_cols if 'id' in col.lower()]\n",
    "                    \n",
    "                    print(f\"Spread ID columns: {spread_id_cols}\")\n",
    "                    print(f\"Scores ID columns: {scores_id_cols}\")\n",
    "                    \n",
    "                    # Try each combination\n",
    "                    for s_col in scores_id_cols:\n",
    "                        for sr_col in spread_id_cols:\n",
    "                            print(f\"Trying join: scores.{s_col} = spread_results.{sr_col}\")\n",
    "                            join_query = f\"\"\"\n",
    "                            SELECT \n",
    "                                s.id,\n",
    "                                s.home_score,\n",
    "                                s.away_score,\n",
    "                                sr.fpi_spread\n",
    "                            FROM \n",
    "                                scores s\n",
    "                            JOIN \n",
    "                                spread_results sr ON s.{s_col} = sr.{sr_col}\n",
    "                            LIMIT 10\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            try:\n",
    "                                join_df = pd.read_sql(join_query, engine)\n",
    "                                if len(join_df) > 0:\n",
    "                                    print(f\"SUCCESS! Found {len(join_df)} records with join: scores.{s_col} = spread_results.{sr_col}\")\n",
    "                                    \n",
    "                                    # Get all records with this join\n",
    "                                    full_query = f\"\"\"\n",
    "                                    SELECT \n",
    "                                        s.id,\n",
    "                                        s.home_score,\n",
    "                                        s.away_score,\n",
    "                                        sr.fpi_spread\n",
    "                                    FROM \n",
    "                                        scores s\n",
    "                                    JOIN \n",
    "                                        spread_results sr ON s.{s_col} = sr.{sr_col}\n",
    "                                    \"\"\"\n",
    "                                    \n",
    "                                    data_df1 = pd.read_sql(full_query, engine)\n",
    "                                    print(f\"Retrieved total of {len(data_df1)} records with this join\")\n",
    "                                    break\n",
    "                            except:\n",
    "                                print(f\"Join failed for scores.{s_col} = spread_results.{sr_col}\")\n",
    "                        \n",
    "                        if len(data_df1) > 0:\n",
    "                            break\n",
    "            \n",
    "            # If we have data, calculate homefield advantage\n",
    "            if len(data_df1) > 0:\n",
    "                print(\"\\nCalculating homefield advantage...\")\n",
    "                \n",
    "                # Calculate actual spread (away_score - home_score)\n",
    "                data_df1['actual_spread'] = data_df1['away_score'] - data_df1['home_score']\n",
    "                \n",
    "                # Calculate prediction error (fpi_spread - actual_spread)\n",
    "                data_df1['prediction_error'] = data_df1['fpi_spread'] - data_df1['actual_spread']\n",
    "                \n",
    "                # Calculate homefield advantage (median of prediction errors)\n",
    "                homefield_advantage = data_df1['prediction_error'].median()\n",
    "                \n",
    "                # Additional statistics\n",
    "                statistics = {\n",
    "                    'median_homefield_advantage': homefield_advantage,\n",
    "                    'mean_error': data_df1['prediction_error'].mean(),\n",
    "                    'std_error': data_df1['prediction_error'].std(),\n",
    "                    'sample_size': len(data_df1),\n",
    "                    'min_error': data_df1['prediction_error'].min(),\n",
    "                    'max_error': data_df1['prediction_error'].max(),\n",
    "                    'q1_error': data_df1['prediction_error'].quantile(0.25),\n",
    "                    'q3_error': data_df1['prediction_error'].quantile(0.75)\n",
    "                }\n",
    "                \n",
    "                # Save data to CSV for further analysis\n",
    "                data_df1.to_csv('homefield_advantage_analysis.csv', index=False)\n",
    "                print(\"Data saved to homefield_advantage_analysis.csv\")\n",
    "                \n",
    "                return statistics, data_df1\n",
    "            else:\n",
    "                print(\"No matching data found between spread_results and scores tables.\")\n",
    "                return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Calculating ideal homefield advantage...\")\n",
    "    stats, data = calculate_homefield_advantage()\n",
    "    \n",
    "    if stats:\n",
    "        print(\"\\n===== ESPN FPI Homefield Advantage Analysis =====\")\n",
    "        print(f\"Ideal Homefield Advantage: {stats['median_homefield_advantage']:.2f} points\")\n",
    "        print(f\"Mean Error: {stats['mean_error']:.2f} points\")\n",
    "        print(f\"Standard Deviation: {stats['std_error']:.2f} points\")\n",
    "        print(f\"Sample Size: {stats['sample_size']} games\")\n",
    "        print(f\"Error Range: {stats['min_error']:.2f} to {stats['max_error']:.2f}\")\n",
    "        print(f\"Interquartile Range: {stats['q1_error']:.2f} to {stats['q3_error']:.2f}\")\n",
    "        \n",
    "        print(\"\\nInterpretation:\")\n",
    "        if stats['median_homefield_advantage'] < 0:\n",
    "            print(f\"ESPN FPI is UNDERESTIMATING home field advantage by {abs(stats['median_homefield_advantage']):.2f} points\")\n",
    "            print(f\"To correct this, SUBTRACT {abs(stats['median_homefield_advantage']):.2f} from the FPI spread\")\n",
    "        else:\n",
    "            print(f\"ESPN FPI is OVERESTIMATING home field advantage by {stats['median_homefield_advantage']:.2f} points\")\n",
    "            print(f\"To correct this, ADD {stats['median_homefield_advantage']:.2f} to the FPI spread\")\n",
    "            \n",
    "        print(\"\\nExample Adjustments:\")\n",
    "        print(f\"If FPI spread is -7 (home team favored by 7), adjusted spread would be {-7 + stats['median_homefield_advantage']:.2f}\")\n",
    "        print(f\"If FPI spread is +3.5 (away team favored by 3.5), adjusted spread would be {3.5 + stats['median_homefield_advantage']:.2f}\")\n",
    "        \n",
    "        print(\"\\nAnalysis files saved:\")\n",
    "        print(\"- homefield_advantage_analysis.csv (Full data for further analysis)\")\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b49ca1-09c6-4e2a-ae91-6b0a3bfbaa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing spread prediction accuracy...\n",
      "Establishing SSH tunnel...\n",
      "SSH tunnel established.\n",
      "Database connection established.\n",
      "Tables in database: ['cache', 'cache_locks', 'casinos', 'early_access_signups', 'failed_jobs', 'fpi_ratings', 'games', 'job_batches', 'jobs', 'migrations', 'money_line_results', 'money_lines', 'nba_margins', 'ncaab_margins', 'ncaaf_margins', 'nfl_margins', 'over_under_results', 'over_unders', 'password_reset_tokens', 'scores', 'sessions', 'sports', 'spread_results', 'spreads', 'subscriptions', 'teams', 'users', 'visitors']\n",
      "Found 'spread' column in table: spreads\n",
      "Columns in spread_results table: ['id', 'spread_id', 'score_id', 'result', 'created_at', 'updated_at', 'fpi_spread', 'fpi_adjusted_spread', 'fpi_spread_difference', 'fpi_correctly_predicted', 'fpi_better_than_spread']\n",
      "Columns in scores table: ['id', 'game_id', 'home_score', 'away_score', 'period', 'created_at', 'updated_at', 'home_fpi', 'away_fpi', 'date']\n",
      "Columns in spreads table: ['id', 'game_id', 'casino_id', 'spread', 'home_odds', 'away_odds', 'recorded_at', 'created_at', 'updated_at']\n",
      "Using 3-way join between scores, spread_results, and spreads\n",
      "Using common column id to join spread_results and spreads\n",
      "Executing query: \n",
      "                        SELECT \n",
      "                            s.id,\n",
      "                            s.home_score,\n",
      "                            s.away_score,\n",
      "                            sr.fpi_spread,\n",
      "                            sr.fpi_adjusted_spread,\n",
      "                            sp.spread\n",
      "                        FROM \n",
      "                            scores s\n",
      "                        JOIN \n",
      "                            spread_results sr ON s.id = sr.id\n",
      "                        JOIN\n",
      "                            spreads sp ON sr.id = sp.id\n",
      "                        \n",
      "                            INNER JOIN (\n",
      "                                SELECT id, MAX(created_at) as max_time\n",
      "                                FROM spread_results\n",
      "                                GROUP BY id\n",
      "                            ) latest ON sr.id = latest.id AND sr.created_at = latest.max_time\n",
      "                            \n",
      "                        \n",
      "Retrieved 0 records\n",
      "No data retrieved with the query.\n",
      "Analysis failed. Please check the error messages above.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_spread_accuracy():\n",
    "    # SSH and database connection parameters\n",
    "    ssh_host = '67.205.170.38'\n",
    "    ssh_port = 22\n",
    "    ssh_username = 'forge'\n",
    "    db_username = 'forge'\n",
    "    db_password = 'kzfSSg5QxhEiK1Y22pwY'\n",
    "    db_name = 'forge'\n",
    "    \n",
    "    try:\n",
    "        print(\"Establishing SSH tunnel...\")\n",
    "        with SSHTunnelForwarder(\n",
    "            (ssh_host, ssh_port),\n",
    "            ssh_username=ssh_username,\n",
    "            remote_bind_address=('127.0.0.1', 3306),\n",
    "            local_bind_address=('127.0.0.1', 10022)\n",
    "        ) as tunnel:\n",
    "            print(\"SSH tunnel established.\")\n",
    "            \n",
    "            # Connect to the database through the SSH tunnel\n",
    "            connection_string = f\"mysql+pymysql://{db_username}:{db_password}@127.0.0.1:{tunnel.local_bind_port}/{db_name}\"\n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            print(\"Database connection established.\")\n",
    "            \n",
    "            # First, check all tables in the database\n",
    "            tables_query = \"SHOW TABLES;\"\n",
    "            tables = pd.read_sql(tables_query, engine)\n",
    "            print(\"Tables in database:\", tables.values.flatten().tolist())\n",
    "            \n",
    "            # Find the table that contains 'spread' column\n",
    "            spread_table = None\n",
    "            for table in tables.values.flatten():\n",
    "                try:\n",
    "                    cols_query = f\"SHOW COLUMNS FROM {table};\"\n",
    "                    cols = pd.read_sql(cols_query, engine)\n",
    "                    if 'spread' in cols['Field'].tolist():\n",
    "                        spread_table = table\n",
    "                        print(f\"Found 'spread' column in table: {spread_table}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not spread_table:\n",
    "                print(\"Could not find a table with 'spread' column!\")\n",
    "                \n",
    "            # Check columns in spread_results\n",
    "            spread_results_cols = pd.read_sql(\"SHOW COLUMNS FROM spread_results\", engine)['Field'].tolist()\n",
    "            print(\"Columns in spread_results table:\", spread_results_cols)\n",
    "            \n",
    "            # Check columns in scores\n",
    "            scores_cols = pd.read_sql(\"SHOW COLUMNS FROM scores\", engine)['Field'].tolist()\n",
    "            print(\"Columns in scores table:\", scores_cols)\n",
    "            \n",
    "            # Check columns in spread table if found\n",
    "            if spread_table:\n",
    "                spread_cols = pd.read_sql(f\"SHOW COLUMNS FROM {spread_table}\", engine)['Field'].tolist()\n",
    "                print(f\"Columns in {spread_table} table:\", spread_cols)\n",
    "            \n",
    "            # First, identify any datetime column for closing spreads\n",
    "            time_col = None\n",
    "            for col in spread_results_cols:\n",
    "                if 'created_at' in col.lower() or 'timestamp' in col.lower() or 'time' in col.lower() or 'date' in col.lower():\n",
    "                    time_col = col\n",
    "                    break\n",
    "            \n",
    "            # Try to get data combining all three tables if necessary\n",
    "            query = None\n",
    "            if spread_table and spread_table != 'spread_results':\n",
    "                print(f\"Using 3-way join between scores, spread_results, and {spread_table}\")\n",
    "                \n",
    "                # Find common columns between spread_results and spread_table\n",
    "                common_cols = set(spread_results_cols).intersection(set(spread_cols))\n",
    "                join_col = next((col for col in common_cols if 'id' in col.lower()), None)\n",
    "                \n",
    "                if not join_col:\n",
    "                    print(\"No common ID column found between spread_results and spread_table!\")\n",
    "                    # Try to use scores as intermediary\n",
    "                    scores_spread_common = set(scores_cols).intersection(set(spread_cols))\n",
    "                    scores_sr_common = set(scores_cols).intersection(set(spread_results_cols))\n",
    "                    \n",
    "                    scores_spread_join = next((col for col in scores_spread_common if 'id' in col.lower()), None)\n",
    "                    scores_sr_join = next((col for col in scores_sr_common if 'id' in col.lower()), None)\n",
    "                    \n",
    "                    if scores_spread_join and scores_sr_join:\n",
    "                        print(f\"Using scores as intermediary with joins: scores.{scores_sr_join}=spread_results.{scores_sr_join} and scores.{scores_spread_join}={spread_table}.{scores_spread_join}\")\n",
    "                        \n",
    "                        # Include closing spread filter if we have a time column\n",
    "                        closing_condition = \"\"\n",
    "                        if time_col:\n",
    "                            closing_condition = f\"\"\"\n",
    "                            INNER JOIN (\n",
    "                                SELECT id, MAX({time_col}) as max_time\n",
    "                                FROM spread_results\n",
    "                                GROUP BY id\n",
    "                            ) latest ON sr.id = latest.id AND sr.{time_col} = latest.max_time\n",
    "                            \"\"\"\n",
    "                        \n",
    "                        query = f\"\"\"\n",
    "                        SELECT \n",
    "                            s.id,\n",
    "                            s.home_score,\n",
    "                            s.away_score,\n",
    "                            sr.fpi_spread,\n",
    "                            sr.fpi_adjusted_spread,\n",
    "                            sp.spread\n",
    "                        FROM \n",
    "                            scores s\n",
    "                        JOIN \n",
    "                            spread_results sr ON s.{scores_sr_join} = sr.{scores_sr_join}\n",
    "                        JOIN\n",
    "                            {spread_table} sp ON s.{scores_spread_join} = sp.{scores_spread_join}\n",
    "                        {closing_condition}\n",
    "                        \"\"\"\n",
    "                else:\n",
    "                    print(f\"Using common column {join_col} to join spread_results and {spread_table}\")\n",
    "                    \n",
    "                    # First find how to join scores and spread_results\n",
    "                    scores_sr_common = set(scores_cols).intersection(set(spread_results_cols))\n",
    "                    scores_sr_join = next((col for col in scores_sr_common if 'id' in col.lower()), None)\n",
    "                    \n",
    "                    if scores_sr_join:\n",
    "                        # Include closing spread filter if we have a time column\n",
    "                        closing_condition = \"\"\n",
    "                        if time_col:\n",
    "                            closing_condition = f\"\"\"\n",
    "                            INNER JOIN (\n",
    "                                SELECT id, MAX({time_col}) as max_time\n",
    "                                FROM spread_results\n",
    "                                GROUP BY id\n",
    "                            ) latest ON sr.id = latest.id AND sr.{time_col} = latest.max_time\n",
    "                            \"\"\"\n",
    "                        \n",
    "                        query = f\"\"\"\n",
    "                        SELECT \n",
    "                            s.id,\n",
    "                            s.home_score,\n",
    "                            s.away_score,\n",
    "                            sr.fpi_spread,\n",
    "                            sr.fpi_adjusted_spread,\n",
    "                            sp.spread\n",
    "                        FROM \n",
    "                            scores s\n",
    "                        JOIN \n",
    "                            spread_results sr ON s.{scores_sr_join} = sr.{scores_sr_join}\n",
    "                        JOIN\n",
    "                            {spread_table} sp ON sr.{join_col} = sp.{join_col}\n",
    "                        {closing_condition}\n",
    "                        \"\"\"\n",
    "            else:\n",
    "                # The simplest case: try different joins between scores and spread_results\n",
    "                print(\"Trying direct join between scores and spread_results\")\n",
    "                potential_joins = [\n",
    "                    \"s.id = sr.score_id\",\n",
    "                    \"s.game_id = sr.game_id\",\n",
    "                    \"s.id = sr.id\",\n",
    "                    \"s.id = sr.spread_id\"\n",
    "                ]\n",
    "                \n",
    "                for join in potential_joins:\n",
    "                    try:\n",
    "                        # Include closing spread filter if we have a time column\n",
    "                        closing_condition = \"\"\n",
    "                        if time_col:\n",
    "                            closing_condition = f\"\"\"\n",
    "                            INNER JOIN (\n",
    "                                SELECT id, MAX({time_col}) as max_time\n",
    "                                FROM spread_results\n",
    "                                GROUP BY id\n",
    "                            ) latest ON sr.id = latest.id AND sr.{time_col} = latest.max_time\n",
    "                            \"\"\"\n",
    "                            \n",
    "                        test_query = f\"\"\"\n",
    "                        SELECT \n",
    "                            COUNT(*)\n",
    "                        FROM \n",
    "                            scores s\n",
    "                        JOIN \n",
    "                            spread_results sr ON {join}\n",
    "                        {closing_condition}\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        count = pd.read_sql(test_query, engine).iloc[0, 0]\n",
    "                        if count > 0:\n",
    "                            print(f\"Found {count} matches with join: {join}\")\n",
    "                            \n",
    "                            # Modify query to include all needed fields\n",
    "                            # Check if adjusted_fpi_spread exists\n",
    "                            fpi_adjustment = \", sr.fpi_adjusted_spread\" if \"adjusted_fpi_spread\" in spread_results_cols else \"\"\n",
    "                            \n",
    "                            query = f\"\"\"\n",
    "                            SELECT \n",
    "                                s.id,\n",
    "                                s.home_score,\n",
    "                                s.away_score,\n",
    "                                sr.fpi_spread{fpi_adjustment}\n",
    "                            FROM \n",
    "                                scores s\n",
    "                            JOIN \n",
    "                                spread_results sr ON {join}\n",
    "                            {closing_condition}\n",
    "                            \"\"\"\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"No matches with join: {join}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with join {join}: {e}\")\n",
    "            \n",
    "            if not query:\n",
    "                print(\"Could not determine appropriate query!\")\n",
    "                return None\n",
    "                \n",
    "            print(\"Executing query:\", query)\n",
    "            data_df = pd.read_sql(query, engine)\n",
    "            print(f\"Retrieved {len(data_df)} records\")\n",
    "            \n",
    "            if len(data_df) == 0:\n",
    "                print(\"No data retrieved with the query.\")\n",
    "                return None\n",
    "                \n",
    "            print(\"Analyzing spread prediction accuracy...\")\n",
    "            \n",
    "            # Calculate actual spread\n",
    "            data_df['actual_spread'] = data_df['away_score'] - data_df['home_score']\n",
    "            \n",
    "            # Calculate error for FPI spread\n",
    "            data_df['fpi_error'] = abs(data_df['fpi_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # If we have vegas spread, calculate its error\n",
    "            if 'spread' in data_df.columns:\n",
    "                data_df['vegas_error'] = abs(data_df['spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Use existing fpi_adjusted_spread if available, otherwise create it\n",
    "            if \"fpi_adjusted_spread\" not in data_df.columns:\n",
    "                data_df['fpi_adjusted_spread'] = data_df['fpi_spread'] - 3.5\n",
    "            \n",
    "            data_df['adjusted_fpi_error'] = abs(data_df['fpi_adjusted_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Calculate absolute difference between adjusted FPI and actual spread for bucketing\n",
    "            data_df['abs_diff'] = abs(data_df['fpi_adjusted_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Create buckets as requested\n",
    "            data_df['diff_bucket'] = pd.cut(\n",
    "                data_df['abs_diff'],\n",
    "                bins=[0, 1, 2, 3, 4, float('inf')],\n",
    "                labels=['0-1', '1-2', '2-3', '3-4', '4+']\n",
    "            )\n",
    "            \n",
    "            # Prepare results\n",
    "            results = {\n",
    "                'total_games': len(data_df),\n",
    "                'fpi_mean_error': data_df['fpi_error'].mean(),\n",
    "                'fpi_adjusted_mean_error': data_df['adjusted_fpi_error'].mean(),\n",
    "            }\n",
    "            \n",
    "            if 'spread' in data_df.columns:\n",
    "                results['vegas_mean_error'] = data_df['vegas_error'].mean()\n",
    "            \n",
    "            # Accuracy by bucket - different versions depending on whether we have vegas spread\n",
    "            if 'spread' in data_df.columns:\n",
    "                bucket_analysis = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'vegas_error': 'mean',\n",
    "                    'fpi_error': 'mean',\n",
    "                    'adjusted_fpi_error': 'mean'\n",
    "                }).rename(columns={'id': 'games'})\n",
    "            else:\n",
    "                bucket_analysis = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'fpi_error': 'mean',\n",
    "                    'adjusted_fpi_error': 'mean'\n",
    "                }).rename(columns={'id': 'games'})\n",
    "            \n",
    "            # Calculate winner prediction accuracy\n",
    "            data_df['fpi_correct_winner'] = np.sign(data_df['fpi_spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "            data_df['adjusted_fpi_correct_winner'] = np.sign(data_df['fpi_adjusted_spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "            \n",
    "            winner_accuracy = {\n",
    "                'fpi_winner_accuracy': data_df['fpi_correct_winner'].mean() * 100,\n",
    "                'fpi_adjusted_winner_accuracy': data_df['adjusted_fpi_correct_winner'].mean() * 100\n",
    "            }\n",
    "            \n",
    "            if 'spread' in data_df.columns:\n",
    "                data_df['vegas_correct_winner'] = np.sign(data_df['spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "                winner_accuracy['vegas_winner_accuracy'] = data_df['vegas_correct_winner'].mean() * 100\n",
    "            \n",
    "            # ATS performance - only if we have vegas spread\n",
    "            ats_performance = {}\n",
    "            ats_by_bucket = None\n",
    "            \n",
    "            if 'spread' in data_df.columns:\n",
    "                # FPI vs Vegas\n",
    "                data_df['fpi_ats_vegas'] = ((data_df['fpi_spread'] - data_df['spread']) * \n",
    "                                          (data_df['actual_spread'] - data_df['spread'])) > 0\n",
    "                \n",
    "                # Adjusted FPI vs Vegas\n",
    "                data_df['adjusted_fpi_ats_vegas'] = ((data_df['fpi_adjusted_spread'] - data_df['spread']) * \n",
    "                                                   (data_df['actual_spread'] - data_df['spread'])) > 0\n",
    "                \n",
    "                ats_performance = {\n",
    "                    'fpi_ats_vegas': data_df['fpi_ats_vegas'].mean() * 100,\n",
    "                    'fpi_adjusted_ats_vegas': data_df['adjusted_fpi_ats_vegas'].mean() * 100\n",
    "                }\n",
    "                \n",
    "                # ATS by bucket\n",
    "                ats_by_bucket = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'fpi_ats_vegas': 'mean',\n",
    "                    'adjusted_fpi_ats_vegas': 'mean'\n",
    "                }).rename(columns={\n",
    "                    'id': 'games',\n",
    "                    'fpi_ats_vegas': 'fpi_ats_pct',\n",
    "                    'adjusted_fpi_ats_vegas': 'fpi_adjusted_ats_pct'\n",
    "                })\n",
    "                \n",
    "                # Convert to percentages\n",
    "                ats_by_bucket['fpi_ats_pct'] = ats_by_bucket['fpi_ats_pct'] * 100\n",
    "                ats_by_bucket['fpi_adjusted_ats_pct'] = ats_by_bucket['fpi_adjusted_ats_pct'] * 100\n",
    "                \n",
    "                # Save to CSV\n",
    "                ats_by_bucket.to_csv('ats_performance_by_bucket.csv')\n",
    "            \n",
    "            # Save results to CSV\n",
    "            data_df.to_csv('spread_accuracy_analysis.csv', index=False)\n",
    "            bucket_analysis.to_csv('spread_accuracy_by_bucket.csv')\n",
    "            \n",
    "            return {\n",
    "                'data': data_df,\n",
    "                'overall_results': results,\n",
    "                'bucket_analysis': bucket_analysis,\n",
    "                'winner_accuracy': winner_accuracy,\n",
    "                'ats_performance': ats_performance,\n",
    "                'ats_by_bucket': ats_by_bucket\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Analyzing spread prediction accuracy...\")\n",
    "    results = analyze_spread_accuracy()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n===== Spread Prediction Accuracy Analysis =====\")\n",
    "        print(f\"Total Games Analyzed: {results['overall_results']['total_games']}\")\n",
    "        \n",
    "        print(\"\\nMean Absolute Error:\")\n",
    "        print(f\"FPI Spread: {results['overall_results']['fpi_mean_error']:.2f} points\")\n",
    "        print(f\"Adjusted FPI Spread: {results['overall_results']['fpi_adjusted_mean_error']:.2f} points\")\n",
    "        \n",
    "        if 'vegas_mean_error' in results['overall_results']:\n",
    "            print(f\"Vegas Spread: {results['overall_results']['vegas_mean_error']:.2f} points\")\n",
    "        \n",
    "        print(\"\\nWinner Prediction Accuracy:\")\n",
    "        print(f\"FPI Spread: {results['winner_accuracy']['fpi_winner_accuracy']:.2f}%\")\n",
    "        print(f\"Adjusted FPI Spread: {results['winner_accuracy']['fpi_adjusted_winner_accuracy']:.2f}%\")\n",
    "        \n",
    "        if 'vegas_winner_accuracy' in results['winner_accuracy']:\n",
    "            print(f\"Vegas Spread: {results['winner_accuracy']['vegas_winner_accuracy']:.2f}%\")\n",
    "        \n",
    "        if results['ats_performance']:\n",
    "            print(\"\\nAgainst The Spread (ATS) Performance:\")\n",
    "            print(f\"FPI vs Vegas: {results['ats_performance']['fpi_ats_vegas']:.2f}%\")\n",
    "            print(f\"Adjusted FPI vs Vegas: {results['ats_performance']['fpi_adjusted_ats_vegas']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nAccuracy by Difference Bucket:\")\n",
    "        print(results['bucket_analysis'])\n",
    "        \n",
    "        if results['ats_by_bucket'] is not None:\n",
    "            print(\"\\nATS Performance by Difference Bucket:\")\n",
    "            print(results['ats_by_bucket'])\n",
    "        \n",
    "        print(\"\\nAnalysis files saved:\")\n",
    "        print(\"- spread_accuracy_analysis.csv (Full data)\")\n",
    "        print(\"- spread_accuracy_by_bucket.csv (Error by bucket)\")\n",
    "        \n",
    "        if results['ats_by_bucket'] is not None:\n",
    "            print(\"- ats_performance_by_bucket.csv (ATS performance by bucket)\")\n",
    "        \n",
    "        # Determine which model performed best\n",
    "        models = [\n",
    "            ('FPI', results['overall_results']['fpi_mean_error']),\n",
    "            ('Adjusted FPI', results['overall_results']['fpi_adjusted_mean_error'])\n",
    "        ]\n",
    "        \n",
    "        if 'vegas_mean_error' in results['overall_results']:\n",
    "            models.append(('Vegas', results['overall_results']['vegas_mean_error']))\n",
    "        \n",
    "        best_model = min(models, key=lambda x: x[1])\n",
    "        print(f\"\\nMost Accurate Model: {best_model[0]} (Mean Error: {best_model[1]:.2f} points)\")\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4523edf9-a9cc-4c53-ae55-0d0b250e2ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing spread prediction accuracy...\n",
      "Establishing SSH tunnel...\n",
      "SSH tunnel established.\n",
      "Database connection established.\n",
      "Getting data from spread_results and scores tables...\n",
      "Retrieved 36724 records from spread_results and scores\n",
      "Getting vegas spreads data...\n",
      "Retrieved 645 vegas spreads records\n",
      "Merged data has 38803 records\n",
      "Analyzing spread prediction accuracy...\n",
      "\n",
      "===== Spread Prediction Accuracy Analysis =====\n",
      "Total Games Analyzed: 38803\n",
      "\n",
      "Mean Absolute Error:\n",
      "FPI Spread: 14.29 points\n",
      "FPI Adjusted Spread: 15.25 points\n",
      "Vegas Spread: 19.61 points\n",
      "\n",
      "Winner Prediction Accuracy:\n",
      "FPI Spread: 30.49%\n",
      "FPI Adjusted Spread: 26.65%\n",
      "Vegas Spread: 13.89%\n",
      "\n",
      "Against The Spread (ATS) Performance:\n",
      "FPI vs Vegas: 69.90%\n",
      "FPI Adjusted vs Vegas: 66.00%\n",
      "\n",
      "Accuracy by Difference Bucket:\n",
      "             games  vegas_error  fpi_error  fpi_adjusted_error\n",
      "diff_bucket                                                   \n",
      "0-1           1476     7.056911   3.463347            0.567954\n",
      "1-2           1120     6.463393   3.946964            1.470714\n",
      "2-3           1073     8.201771   3.473159            2.582293\n",
      "3-4           1343     6.592331   3.262249            3.488235\n",
      "4+           28605    23.158381  16.180811           17.572491\n",
      "\n",
      "ATS Performance by Difference Bucket:\n",
      "             games  fpi_ats_pct  fpi_adjusted_ats_pct\n",
      "diff_bucket                                          \n",
      "0-1           1476    87.669377             96.612466\n",
      "1-2           1120    92.589286             89.107143\n",
      "2-3           1073    87.884436             83.410997\n",
      "3-4           1343    68.726731             84.810127\n",
      "4+           28605    80.153819             73.948610\n",
      "\n",
      "Predictive Metrics:\n",
      "FPI Correctly Predicted: 29.68%\n",
      "FPI Better Than Spread: 52.88%\n",
      "\n",
      "Predictive Metrics by Bucket:\n",
      "             games  fpi_correctly_predicted_pct  fpi_better_than_spread_pct\n",
      "diff_bucket                                                                \n",
      "0-1           1476                   100.000000                   85.569106\n",
      "1-2           1120                    82.857143                   52.500000\n",
      "2-3           1073                    90.027959                   71.575023\n",
      "3-4           1343                    68.056590                   45.048399\n",
      "4+           28605                    19.902115                   50.879217\n",
      "\n",
      "Analysis files saved:\n",
      "- spread_accuracy_analysis.csv (Full data)\n",
      "- spread_accuracy_by_bucket.csv (Error by bucket)\n",
      "- ats_performance_by_bucket.csv (ATS performance by bucket)\n",
      "- predictive_metrics_by_bucket.csv (Predictive metrics by bucket)\n",
      "\n",
      "Most Accurate Model: FPI (Mean Error: 14.29 points)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_spread_accuracy():\n",
    "    # SSH and database connection parameters\n",
    "    ssh_host = '67.205.170.38'\n",
    "    ssh_port = 22\n",
    "    ssh_username = 'forge'\n",
    "    db_username = 'forge'\n",
    "    db_password = 'kzfSSg5QxhEiK1Y22pwY'\n",
    "    db_name = 'forge'\n",
    "    \n",
    "    try:\n",
    "        print(\"Establishing SSH tunnel...\")\n",
    "        with SSHTunnelForwarder(\n",
    "            (ssh_host, ssh_port),\n",
    "            ssh_username=ssh_username,\n",
    "            remote_bind_address=('127.0.0.1', 3306),\n",
    "            local_bind_address=('127.0.0.1', 10022)\n",
    "        ) as tunnel:\n",
    "            print(\"SSH tunnel established.\")\n",
    "            \n",
    "            # Connect to the database through the SSH tunnel\n",
    "            connection_string = f\"mysql+pymysql://{db_username}:{db_password}@127.0.0.1:{tunnel.local_bind_port}/{db_name}\"\n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            print(\"Database connection established.\")\n",
    "            \n",
    "            # From the diagnostic output, we know:\n",
    "            # 1. spread_results.score_id joins to scores.id (36724 matches)\n",
    "            # 2. spreads.game_id joins to scores.game_id (for vegas spreads)\n",
    "            # 3. fpi_spread and fpi_adjusted_spread are in spread_results table\n",
    "            \n",
    "            # Get the data - join spread_results to scores\n",
    "            print(\"Getting data from spread_results and scores tables...\")\n",
    "            \n",
    "            query = \"\"\"\n",
    "            SELECT \n",
    "                sc.id,\n",
    "                sc.home_score,\n",
    "                sc.away_score,\n",
    "                sr.fpi_spread,\n",
    "                sr.fpi_adjusted_spread,\n",
    "                sr.fpi_correctly_predicted,\n",
    "                sr.fpi_better_than_spread\n",
    "            FROM \n",
    "                scores sc\n",
    "            JOIN \n",
    "                spread_results sr ON sc.id = sr.score_id\n",
    "            \"\"\"\n",
    "            \n",
    "            data_df = pd.read_sql(query, engine)\n",
    "            print(f\"Retrieved {len(data_df)} records from spread_results and scores\")\n",
    "            \n",
    "            # Optionally get vegas spreads - join scores to spreads\n",
    "            # Get closing lines by using MAX(recorded_at)\n",
    "            print(\"Getting vegas spreads data...\")\n",
    "            \n",
    "            vegas_query = \"\"\"\n",
    "            SELECT \n",
    "                sc.id as score_id,\n",
    "                sp.spread as vegas_spread\n",
    "            FROM \n",
    "                scores sc\n",
    "            JOIN \n",
    "                spreads sp ON sc.game_id = sp.game_id\n",
    "            JOIN (\n",
    "                SELECT \n",
    "                    game_id, \n",
    "                    MAX(recorded_at) as max_time\n",
    "                FROM \n",
    "                    spreads\n",
    "                GROUP BY \n",
    "                    game_id\n",
    "            ) latest ON sp.game_id = latest.game_id AND sp.recorded_at = latest.max_time\n",
    "            \"\"\"\n",
    "            \n",
    "            vegas_df = pd.read_sql(vegas_query, engine)\n",
    "            print(f\"Retrieved {len(vegas_df)} vegas spreads records\")\n",
    "            \n",
    "            # Merge vegas data if available\n",
    "            if len(vegas_df) > 0:\n",
    "                data_df = pd.merge(data_df, vegas_df, left_on='id', right_on='score_id', how='left')\n",
    "                print(f\"Merged data has {len(data_df)} records\")\n",
    "            \n",
    "            print(\"Analyzing spread prediction accuracy...\")\n",
    "            \n",
    "            # Calculate actual spread (away_score - home_score)\n",
    "            data_df['actual_spread'] = data_df['away_score'] - data_df['home_score']\n",
    "            \n",
    "            # Calculate error for FPI spread\n",
    "            data_df['fpi_error'] = abs(data_df['fpi_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Calculate error for adjusted FPI spread\n",
    "            data_df['fpi_adjusted_error'] = abs(data_df['fpi_adjusted_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Calculate error for vegas if available\n",
    "            if 'vegas_spread' in data_df.columns:\n",
    "                data_df['vegas_error'] = abs(data_df['vegas_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Calculate absolute difference between adjusted FPI and actual spread for bucketing\n",
    "            data_df['abs_diff'] = abs(data_df['fpi_adjusted_spread'] - data_df['actual_spread'])\n",
    "            \n",
    "            # Create smaller buckets as requested\n",
    "            data_df['diff_bucket'] = pd.cut(\n",
    "                data_df['abs_diff'],\n",
    "                bins=[0, 1, 2, 3, 4, float('inf')],\n",
    "                labels=['0-1', '1-2', '2-3', '3-4', '4+']\n",
    "            )\n",
    "            \n",
    "            # Overall accuracy metrics\n",
    "            results = {\n",
    "                'total_games': len(data_df),\n",
    "                'fpi_mean_error': data_df['fpi_error'].mean(),\n",
    "                'fpi_adjusted_mean_error': data_df['fpi_adjusted_error'].mean()\n",
    "            }\n",
    "            \n",
    "            if 'vegas_error' in data_df.columns:\n",
    "                results['vegas_mean_error'] = data_df['vegas_error'].mean()\n",
    "            \n",
    "            # Accuracy by bucket\n",
    "            if 'vegas_error' in data_df.columns:\n",
    "                bucket_analysis = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'vegas_error': 'mean',\n",
    "                    'fpi_error': 'mean',\n",
    "                    'fpi_adjusted_error': 'mean'\n",
    "                }).rename(columns={'id': 'games'})\n",
    "            else:\n",
    "                bucket_analysis = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'fpi_error': 'mean',\n",
    "                    'fpi_adjusted_error': 'mean'\n",
    "                }).rename(columns={'id': 'games'})\n",
    "            \n",
    "            # Calculate winner prediction accuracy\n",
    "            data_df['fpi_correct_winner'] = np.sign(data_df['fpi_spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "            data_df['fpi_adjusted_correct_winner'] = np.sign(data_df['fpi_adjusted_spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "            \n",
    "            winner_accuracy = {\n",
    "                'fpi_winner_accuracy': data_df['fpi_correct_winner'].mean() * 100,\n",
    "                'fpi_adjusted_winner_accuracy': data_df['fpi_adjusted_correct_winner'].mean() * 100\n",
    "            }\n",
    "            \n",
    "            if 'vegas_spread' in data_df.columns:\n",
    "                data_df['vegas_correct_winner'] = np.sign(data_df['vegas_spread']) * np.sign(data_df['actual_spread']) >= 0\n",
    "                winner_accuracy['vegas_winner_accuracy'] = data_df['vegas_correct_winner'].mean() * 100\n",
    "            \n",
    "            # ATS performance (if vegas data available)\n",
    "            ats_performance = {}\n",
    "            ats_by_bucket = None\n",
    "            \n",
    "            if 'vegas_spread' in data_df.columns:\n",
    "                # FPI vs Vegas\n",
    "                data_df['fpi_ats_vegas'] = ((data_df['fpi_spread'] - data_df['vegas_spread']) * \n",
    "                                          (data_df['actual_spread'] - data_df['vegas_spread'])) > 0\n",
    "                \n",
    "                # Adjusted FPI vs Vegas\n",
    "                data_df['fpi_adjusted_ats_vegas'] = ((data_df['fpi_adjusted_spread'] - data_df['vegas_spread']) * \n",
    "                                                   (data_df['actual_spread'] - data_df['vegas_spread'])) > 0\n",
    "                \n",
    "                ats_performance = {\n",
    "                    'fpi_ats_vegas': data_df['fpi_ats_vegas'].mean() * 100,\n",
    "                    'fpi_adjusted_ats_vegas': data_df['fpi_adjusted_ats_vegas'].mean() * 100\n",
    "                }\n",
    "                \n",
    "                # ATS by bucket\n",
    "                ats_by_bucket = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'fpi_ats_vegas': 'mean',\n",
    "                    'fpi_adjusted_ats_vegas': 'mean'\n",
    "                }).rename(columns={\n",
    "                    'id': 'games',\n",
    "                    'fpi_ats_vegas': 'fpi_ats_pct',\n",
    "                    'fpi_adjusted_ats_vegas': 'fpi_adjusted_ats_pct'\n",
    "                })\n",
    "                \n",
    "                # Convert to percentages\n",
    "                ats_by_bucket['fpi_ats_pct'] = ats_by_bucket['fpi_ats_pct'] * 100\n",
    "                ats_by_bucket['fpi_adjusted_ats_pct'] = ats_by_bucket['fpi_adjusted_ats_pct'] * 100\n",
    "                \n",
    "                # Save to CSV\n",
    "                ats_by_bucket.to_csv('ats_performance_by_bucket.csv')\n",
    "            \n",
    "            # Save results to CSV\n",
    "            data_df.to_csv('spread_accuracy_analysis.csv', index=False)\n",
    "            bucket_analysis.to_csv('spread_accuracy_by_bucket.csv')\n",
    "            \n",
    "            # Also analyze predictive accuracy with the existing flags\n",
    "            if 'fpi_correctly_predicted' in data_df.columns and 'fpi_better_than_spread' in data_df.columns:\n",
    "                predictive_metrics = {\n",
    "                    'fpi_correctly_predicted_pct': data_df['fpi_correctly_predicted'].mean() * 100,\n",
    "                    'fpi_better_than_spread_pct': data_df['fpi_better_than_spread'].mean() * 100\n",
    "                }\n",
    "                \n",
    "                # Predictive metrics by bucket\n",
    "                predictive_by_bucket = data_df.groupby('diff_bucket').agg({\n",
    "                    'id': 'count',\n",
    "                    'fpi_correctly_predicted': 'mean',\n",
    "                    'fpi_better_than_spread': 'mean'\n",
    "                }).rename(columns={\n",
    "                    'id': 'games',\n",
    "                    'fpi_correctly_predicted': 'fpi_correctly_predicted_pct',\n",
    "                    'fpi_better_than_spread': 'fpi_better_than_spread_pct'\n",
    "                })\n",
    "                \n",
    "                # Convert to percentages\n",
    "                predictive_by_bucket['fpi_correctly_predicted_pct'] = predictive_by_bucket['fpi_correctly_predicted_pct'] * 100\n",
    "                predictive_by_bucket['fpi_better_than_spread_pct'] = predictive_by_bucket['fpi_better_than_spread_pct'] * 100\n",
    "                \n",
    "                # Save to CSV\n",
    "                predictive_by_bucket.to_csv('predictive_metrics_by_bucket.csv')\n",
    "            else:\n",
    "                predictive_metrics = None\n",
    "                predictive_by_bucket = None\n",
    "            \n",
    "            return {\n",
    "                'data': data_df,\n",
    "                'overall_results': results,\n",
    "                'bucket_analysis': bucket_analysis,\n",
    "                'winner_accuracy': winner_accuracy,\n",
    "                'ats_performance': ats_performance,\n",
    "                'ats_by_bucket': ats_by_bucket,\n",
    "                'predictive_metrics': predictive_metrics,\n",
    "                'predictive_by_bucket': predictive_by_bucket\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Analyzing spread prediction accuracy...\")\n",
    "    results = analyze_spread_accuracy()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n===== Spread Prediction Accuracy Analysis =====\")\n",
    "        print(f\"Total Games Analyzed: {results['overall_results']['total_games']}\")\n",
    "        \n",
    "        print(\"\\nMean Absolute Error:\")\n",
    "        print(f\"FPI Spread: {results['overall_results']['fpi_mean_error']:.2f} points\")\n",
    "        print(f\"FPI Adjusted Spread: {results['overall_results']['fpi_adjusted_mean_error']:.2f} points\")\n",
    "        \n",
    "        if 'vegas_mean_error' in results['overall_results']:\n",
    "            print(f\"Vegas Spread: {results['overall_results']['vegas_mean_error']:.2f} points\")\n",
    "        \n",
    "        print(\"\\nWinner Prediction Accuracy:\")\n",
    "        print(f\"FPI Spread: {results['winner_accuracy']['fpi_winner_accuracy']:.2f}%\")\n",
    "        print(f\"FPI Adjusted Spread: {results['winner_accuracy']['fpi_adjusted_winner_accuracy']:.2f}%\")\n",
    "        \n",
    "        if 'vegas_winner_accuracy' in results['winner_accuracy']:\n",
    "            print(f\"Vegas Spread: {results['winner_accuracy']['vegas_winner_accuracy']:.2f}%\")\n",
    "        \n",
    "        if results['ats_performance']:\n",
    "            print(\"\\nAgainst The Spread (ATS) Performance:\")\n",
    "            print(f\"FPI vs Vegas: {results['ats_performance']['fpi_ats_vegas']:.2f}%\")\n",
    "            print(f\"FPI Adjusted vs Vegas: {results['ats_performance']['fpi_adjusted_ats_vegas']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nAccuracy by Difference Bucket:\")\n",
    "        print(results['bucket_analysis'])\n",
    "        \n",
    "        if results['ats_by_bucket'] is not None:\n",
    "            print(\"\\nATS Performance by Difference Bucket:\")\n",
    "            print(results['ats_by_bucket'])\n",
    "        \n",
    "        if results['predictive_metrics'] is not None:\n",
    "            print(\"\\nPredictive Metrics:\")\n",
    "            print(f\"FPI Correctly Predicted: {results['predictive_metrics']['fpi_correctly_predicted_pct']:.2f}%\")\n",
    "            print(f\"FPI Better Than Spread: {results['predictive_metrics']['fpi_better_than_spread_pct']:.2f}%\")\n",
    "            \n",
    "            print(\"\\nPredictive Metrics by Bucket:\")\n",
    "            print(results['predictive_by_bucket'])\n",
    "        \n",
    "        print(\"\\nAnalysis files saved:\")\n",
    "        print(\"- spread_accuracy_analysis.csv (Full data)\")\n",
    "        print(\"- spread_accuracy_by_bucket.csv (Error by bucket)\")\n",
    "        \n",
    "        if results['ats_by_bucket'] is not None:\n",
    "            print(\"- ats_performance_by_bucket.csv (ATS performance by bucket)\")\n",
    "            \n",
    "        if results['predictive_by_bucket'] is not None:\n",
    "            print(\"- predictive_metrics_by_bucket.csv (Predictive metrics by bucket)\")\n",
    "        \n",
    "        # Determine which model performed best\n",
    "        models = [\n",
    "            ('FPI', results['overall_results']['fpi_mean_error']),\n",
    "            ('FPI Adjusted', results['overall_results']['fpi_adjusted_mean_error'])\n",
    "        ]\n",
    "        \n",
    "        if 'vegas_mean_error' in results['overall_results']:\n",
    "            models.append(('Vegas', results['overall_results']['vegas_mean_error']))\n",
    "        \n",
    "        best_model = min(models, key=lambda x: x[1])\n",
    "        print(f\"\\nMost Accurate Model: {best_model[0]} (Mean Error: {best_model[1]:.2f} points)\")\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483143f1-fe72-47d5-8e18-7409c2e03786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed589d5-fd9b-4ef7-b380-aa475350160e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
